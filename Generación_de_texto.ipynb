{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMHmOl2OfMFksrlDR1Cftu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nachodelis/deepLearning/blob/master/Generaci%C3%B3n_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generación de textos de terror"
      ],
      "metadata": {
        "id": "Hl3XmgRwvX_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introducción"
      ],
      "metadata": {
        "id": "xEUDzzwoxXUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este trabajo se ha utilizado un enfoque basado en redes neuronales recurrentes para generar texto a partir de un conjunto de datos de texto existente. El objetivo de este modelo es generar texto que sea coherente y tenga sentido, similar al texto que se encuentra en el conjunto de datos de entrenamiento."
      ],
      "metadata": {
        "id": "6-DTSd0evd3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Código"
      ],
      "metadata": {
        "id": "fdu2teZJxaRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importamos las librerías"
      ],
      "metadata": {
        "id": "Dv3r5locvqvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "metadata": {
        "id": "UTpfBKWtvco4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La primera parte del código implica cargar los datos en lotes (o \"batches\") de 10 filas y luego crear un vocabulario de palabras únicas y asignar un índice único a cada palabra en el conjunto de datos. Se utiliza la función StringLookup de la librería preprocessing de TensorFlow para codificar las palabras del conjunto de datos.\n",
        "\n",
        "A continuación, se convierten todas las letras a minúsculas y se eliminan los signos de puntuación. Después, se reemplaza cada palabra en el conjunto de datos con su índice correspondiente utilizando el codificador de palabras.\n",
        "\n",
        "Luego se ajusta la longitud de cada secuencia de palabras a la longitud máxima, llenando con ceros al final de la secuencia.\n",
        "\n",
        "Se definen los parámetros de la red neuronal recurrente, como la dimensión del embedding y las unidades RNN, y se define la arquitectura de la red neuronal recurrente en sí. El modelo se compila y se entrena en el conjunto de datos cargado previamente.\n",
        "\n",
        "El modelo predecirá la siguiente palabra en una oración en función de las palabras anteriores."
      ],
      "metadata": {
        "id": "yNgPsAvhvxgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# cargamos los datos en batches de 10 filas\n",
        "chunksize = 10\n",
        "for chunk in pd.read_csv('terror.csv', delimiter=None, error_bad_lines=False, nrows=200, chunksize=chunksize):\n",
        "    \n",
        "    # crea un vocabulario de palabras únicas y asigna un índice único a cada palabra\n",
        "    vocab_size = 10000\n",
        "    word_counts = pd.Series(' '.join(chunk['body']).split()).value_counts()\n",
        "    vocab = list(word_counts.iloc[:vocab_size - 1].index)\n",
        "\n",
        "    # crea una capa de codificación de palabras usando preprocessing.StringLookup\n",
        "    encoder = preprocessing.StringLookup(vocabulary=vocab, mask_token=None)\n",
        "\n",
        "    # convierte todas las letras a minúsculas y elimina signos de puntuación\n",
        "    chunk['body'] = chunk['body'].apply(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "    # reemplaza cada palabra en el conjunto de datos con su índice correspondiente usando el codificador de palabras\n",
        "    chunk['body'] = chunk['body'].apply(lambda x: np.array(encoder(x.split())))\n",
        "\n",
        "    # ajustar la longitud de cada secuencia a la longitud máxima llenando con ceros al final de la secuencia\n",
        "    max_length = max(chunk['body'].apply(len))\n",
        "    chunk['body'] = chunk['body'].apply(lambda x: np.pad(x, (0, max_length - len(x)), 'constant'))\n",
        "\n",
        "    # define los parámetros de la red neuronal recurrente\n",
        "    embedding_dim = 256\n",
        "    rnn_units = 100\n",
        "\n",
        "    # define la arquitectura de la red neuronal recurrente\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(encoder.get_vocabulary())+1)\n",
        "])\n",
        "\n",
        "    # compila el modelo\n",
        "    model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  optimizer='adam')\n",
        "\n",
        "    # entrena el modelo\n",
        "    batch_size = 5\n",
        "    sequence_length = max_length\n",
        "    input_data = np.array(chunk['body'].tolist())[:, :-1]  # Todas las palabras menos la última\n",
        "    output_data = np.array(chunk['body'].tolist())[:, 1:]  # La siguiente palabra para cada palabra de entrada\n",
        "    history = model.fit(input_data, output_data, batch_size=batch_size, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtW6ubjDwnFL",
        "outputId": "7b5df828-686e-43d1-eff3-9dbdb1c907a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-106e71fb8d45>:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  for chunk in pd.read_csv('terror.csv', delimiter=None, error_bad_lines=False, nrows=200, chunksize=chunksize):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 28s 12s/step - loss: 8.7100\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 8.6803\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 8.6262\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 8.4962\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 8.2536\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 7.9549\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 7.6472\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 25s 13s/step - loss: 7.3545\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 23s 12s/step - loss: 7.0414\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 6.7173\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 6.3730\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 6.0508\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 5.7224\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 5.4156\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 5.1235\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 4.8636\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 25s 12s/step - loss: 4.6094\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 25s 12s/step - loss: 4.4123\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 25s 12s/step - loss: 4.2269\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 4.0472\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.9134\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 3.7844\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.7206\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6628\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6462\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6360\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6414\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 3.6447\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6493\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6477\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6413\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6335\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6223\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.6122\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 24s 13s/step - loss: 3.6041\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5985\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5949\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5933\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5908\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5908\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 3.5902\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5904\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5889\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5871\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5853\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 24s 12s/step - loss: 3.5843\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 23s 11s/step - loss: 3.5814\n",
            "Epoch 48/50\n",
            "1/2 [==============>...............] - ETA: 11s - loss: 3.9550"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La segunda parte del código utiliza el modelo de red neuronal recurrente entrenado previamente para generar texto a partir de una secuencia de palabras iniciales. En este caso, se especifica una lista de tres palabras de inicio, y se establece la variable num_words en 50 para generar un total de 50 palabras adicionales.\n",
        "\n",
        "Primero, las palabras de inicio se convierten en una secuencia de índices utilizando el codificador de palabras creado previamente. Esto se hace para que el modelo pueda procesar las palabras como entrada y generar nuevas palabras como salida.\n",
        "\n",
        "Luego, se utiliza un bucle for para generar num_words palabras adicionales. En cada iteración del bucle, se predice la palabra siguiente dada la secuencia actual de palabras utilizando el modelo de red neuronal recurrente previamente entrenado. Esto se hace utilizando el método predict del modelo para obtener las predicciones de salida.\n",
        "\n",
        "La predicción se convierte en un índice de palabra utilizando el método tf.random.categorical, que selecciona un índice al azar de las distribuciones de probabilidad de salida. La palabra predicha se agrega a la secuencia de entrada actual, y el proceso se repite para generar la siguiente palabra.\n",
        "\n",
        "Finalmente, la secuencia de índices de palabras generada se convierte de nuevo en palabras utilizando el codificador de palabras invertido, y se muestra el resultado utilizando el método join de Python para unir las palabras en una cadena de texto."
      ],
      "metadata": {
        "id": "wJYEiwRYwsL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar texto de tres palabras de inicio\n",
        "start_words = ['this', 'job', 'is']\n",
        "num_words = 50\n",
        "\n",
        "# Convierte las palabras de inicio en una secuencia de índices usando el codificador de palabras\n",
        "start_sequence = np.array(encoder(start_words)).reshape(1, -1)\n",
        "\n",
        "# Genera num_words palabras adicionales usando el modelo\n",
        "for i in range(num_words):\n",
        "    # Predice la palabra siguiente dada la secuencia actual de palabras\n",
        "    predicted_logits = model.predict(start_sequence)[:, -1, :]\n",
        "    predicted_id = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    # Añade la palabra predicha a la secuencia de entrada\n",
        "    start_sequence = np.concatenate([start_sequence, predicted_id], axis=-1)\n",
        "\n",
        "# Convierte la secuencia de índices en palabras y muestra el resultado\n",
        "predicted_words = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "                    vocabulary=encoder.get_vocabulary(), invert=True)(start_sequence)[0].numpy()\n",
        "predicted_words = predicted_words.reshape((-1,))\n",
        "\n",
        "predicted_words = [word.decode('utf-8') for word in predicted_words]\n",
        "print(' '.join(predicted_words))\n"
      ],
      "metadata": {
        "id": "eHT2YjgZw_E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusión"
      ],
      "metadata": {
        "id": "_i95LXLexS_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "En conclusión, el código muestra cómo entrenar un modelo de lenguaje utilizando redes neuronales recurrentes para generar texto a partir de un corpus de datos. Utilizando el conjunto de datos de terror como ejemplo, se utiliza el preprocesamiento de datos, la codificación de palabras y la definición de la arquitectura de la red neuronal para entrenar el modelo. El modelo se usa para generar texto a partir de una secuencia de palabras de inicio dadas y se muestra cómo se puede convertir la secuencia de índices generada en palabras para mostrar el resultado final."
      ],
      "metadata": {
        "id": "nVYeeztcxROM"
      }
    }
  ]
}